{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9109bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmaxAgent:\n",
    "\n",
    "    def __init__(self, env, R_max, gamma, max_visits_per_state, max_episodes, max_steps, epsilon=0.2):\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration probability\n",
    "        self.max_steps = max_steps  # max steps allowed in each episode\n",
    "        self.max_visits_per_state = max_visits_per_state  # number of times to visit each state-action pair\n",
    "        self.max_episodes = max_episodes  # maximum number of episodes to run\n",
    "        self.Q = np.ones((nS, nA)) * R_max / (1 - self.gamma)  # Q table initialized with optimistic rewards\n",
    "        # self.Q = np.random.random((nS, nA))  # Q table initialized with random values\n",
    "        self.R = np.zeros((nS, nA))  # reward table for each state-action pair\n",
    "        self.nSA = np.zeros((nS, nA))  # counter for each state-action pair\n",
    "        self.nSAS = np.zeros((nS, nA, nS))  # counter for each state-action-next state triple\n",
    "        self.val1 = []  # list to store mean rewards per 500 episodes\n",
    "        self.val2 = []  # list to store episode numbers for mean rewards calculation\n",
    "        print(int(np.ceil(np.log(1 / (self.epsilon * (1 - self.gamma))) / (1 - self.gamma)))) # print exploration depth\n",
    "\n",
    "\n",
    "    def estimate_transition_probabilities(self):\n",
    "        for episode in range(self.max_episodes):\n",
    "            obs = env.reset()\n",
    "            if episode % 20 == 0:\n",
    "                self.val1.append(self.mean_rewards_per_500())\n",
    "                self.val2.append(episode)\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                best_action = self.choose_action(obs)\n",
    "                new_obs, reward, done, _ = env.step(best_action)\n",
    "\n",
    "                if self.nSA[obs][best_action] < self.max_visits_per_state:\n",
    "                    self.nSA[obs][best_action] += 1\n",
    "                    self.R[obs][best_action] += reward\n",
    "                    self.nSAS[obs][best_action][new_obs] += 1\n",
    "\n",
    "                    if self.nSA[obs][best_action] == self.max_visits_per_state:\n",
    "                        for i in range(int(np.ceil(np.log(1 / (self.epsilon * (1 - self.gamma))) / (1 - self.gamma)))):\n",
    "\n",
    "                            for state in range(nS):\n",
    "                                for action in range(nA):\n",
    "                                    if self.nSA[state][action] >= self.max_visits_per_state:\n",
    "                                        # In the cited paper it is given that reward[s,a]= summation of rewards / nSA[s,a]\n",
    "                                        # We have already calculated the summation of rewards in line 28\n",
    "                                        q = (self.R[state][action] / self.nSA[state][action])\n",
    "\n",
    "                                        for next_state in range(nS):\n",
    "                                            # In the cited paper it is given that transition[s,a] = nSAS'[s,a,s']/nSA[s,a]\n",
    "                                            transition = self.nSAS[state][action][next_state] / self.nSA[state][action]\n",
    "                                            q += (transition * np.max(self.Q[next_state, :]))\n",
    "\n",
    "                                        self.Q[state][action] = q\n",
    "\n",
    "                if done:\n",
    "                    if not(reward == 1):\n",
    "                        self.R[obs][best_action] = -10\n",
    "                    break\n",
    "\n",
    "                obs = new_obs\n",
    "\n",
    "\n",
    "    def mean_rewards_per_500(self):\n",
    "        total_reward = 0\n",
    "        for episodes in range(500):\n",
    "            observation = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = self.choose_action(observation)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    observation = env.reset()\n",
    "                    break\n",
    "        return (total_reward/500) \n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > (1-self.epsilon):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[observation])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb19bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmaxagent = RmaxAgent(env, 1 , 0.98, 25, 500, 10)\n",
    "env.close()\n",
    "#rmaxagent.mean_rewards_per_500()\n",
    "rmaxagent.estimate_transition_probabilities()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rmaxagent.val2[::7],rmaxagent.val1[::7])\n",
    "plt.xlabel(\"Number of Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"R-Max on a 4x4 environment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
